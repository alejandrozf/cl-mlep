;;;;; -*- Mode: Lisp; Syntax: ANSI-Common-Lisp; Base: 10 -*-

(in-package :mlep)

(defgeneric run (instance &key)
  (:documentation "@arg[instance]{an instance of any @code{mlep} learning algorithm}
@arg[threshold]{a threshold that is the minimum global error to be achieved -- iterative training runs until threshold is reached (supported by @code{perceptron})}
@arg[epochs]{number of how often a iterative algorithm should be performed (supported by @code{k-means} and @code{neuronal-network})}
@return{depends on the learning algorithm:
@itemize{
@item{@code{k-means}: the computed means}
@item{@code{k-nearest-neighbors}: the classes assigned to @code{test-set}}
@item{@code{max-likelihood}: a list with the first item being the mean and the second one being the co-variance matrix of the normal distribution}
@item{@code{markov-chain}: the probability matrix (or tensor)}
@item{@code{naive-bayes}: the classes assigned to @code{test-set}}
@item{@code{neuronal-network}: nothing}
@item{@code{perceptron}: nothing}
@item{@code{principal-component-analysis}: list with three matrices - @code{unitary-matrix1 U} (orthogonal matrix), @code{unitary-matrix2 Vt} (orthogonal matrix) and @code{singular-values} (a diagonal matrix with the diagonal elements being the singular values called @code{D}; @code{UxDxVt} should be a reconstruction of the input matrix)}
@item{@code{imputer}: The default replace values for each column.}
}}
A general interface for 'running' a learning algorithm."))

(defgeneric initialize (instance &key)
  (:documentation "Initialize the arguments of INSTANCE."))

(defgeneric classify (instance &key)
  (:documentation "@arg[instance]{an instance of @code{k-means}, @code{perceptron} or @code{neuronal-network}}
@arg[new-data-set]{use @code{new-data-set} instead of the internal @code{data-set}}
@arg[verbose]{print some more information (only taken into account for @code{neuronal-network})}
@return{a list with a classification number according to each sample in the classified data-set}
Classifying some data-set."))

(defgeneric backprop (instance input wanted)
  (:documentation "Adjust weights with backpropagation."))

(defgeneric forward (instance &key)
  (:documentation "@arg[instance]{an instance of @code{neuronal-network}}
@arg[input]{the input data to be considered}
@return{the output of the @code{neuronal-network} given the @code{input}}
Computes a forward path through the network and gives its output."))

(defgeneric synthesize (instance &key)
  (:documentation "@arg[instance]{an instance of @code{markov-chain}}
@arg[start]{1) the symbol @code{random} -- a new random sequence as beginning is generated; 2) @code{nil} (default) -- a literal subsequence with random starting index is taken as beginning, 3) a list with an user given starting sequence}
@arg[howmany]{number of elements to be synthesized}
Synthesize some data."))

(defgeneric analyze (instance input &key)
  (:documentation    
   "@arg[instance]{an instance of @code{markov-chain}}
@arg[input]{some input data}
@return{the probability of @code{input}}
Check the probability of @code{input} being generated by @code{instance}."))

(defgeneric data-set (instance)
  (:documentation "@arg[instance]{an instance of any @code{mlep} learning algorithm}
@return{the data-set of @code{instance}}
Get the data-set of @code{instance}."))

(defgeneric distance (instance)
  (:documentation "@arg[instance]{an instance of @code{k-means} or @code{k-nearest-neighbors}}
@return{the function for calculating the distance for @code{instance}}
Get the function for calculating the distance for @code{instance}, e.g. @code{#'euclidian-distance}."))

(defgeneric k (instance)
  (:documentation "@arg[instance]{an instance of @code{k-means} or @code{k-nearest-neighbors}}
@return{the parameter @code{k}}
@code{k} determines how many means are assumed (for @code{k-means}) resp. how many neighbors are considered (for @code{k-nearest-neighbors})."))

(defgeneric order (instance)
  (:documentation "@arg[instance]{an instance of @code{markov-chain}}
@return{the order of the markov chain}
The order of a markov chain determines how much past events are considered for producing a current event."))

(defgeneric probabilities (instance)
  (:documentation "@arg[instance]{an instance of @code{markov-chain}}
@return{the probabilities of the markov chain}
Get the probability matrix (or tensor) -- it's rank is @code{order+1}."))

(defgeneric set-labels (instance)
  (:documentation "@arg[instance]{an instance of @code{k-nearest-neighbors}, @code{naive-bayes}, @code{neuronal-network} or @code{perceptron}}
@return{the target labels for @code{data-set} of a supervised learning algorithm.}
Get the target labels."))

(defgeneric test-set (instance)
  (:documentation "@arg[instance]{an instance of @code{k-nearest-neighbors} or @code{naive-bayes}}
@return{the test, i.e. a set that has no target labels and needs to be classified.}
Get the test-set."))

(defgeneric unique (instance)
  (:documentation "@arg[instance]{an instance of @code{markov-chain}}
@return{all unique elements in @code{data-set}}
Get all unique values are considered by the chain."))

(defgeneric means (instance)
  (:documentation "@arg[instance]{an instance of @code{k-means} or @code{principal-component-analysis}}
@return{the current means}
Get the current means."))

(defgeneric learning-rate (instance)
  (:documentation "@arg[instance]{an instance of @code{neuronal-network} or @code{perceptron}}
@return{the learning-rate}
The learning rate controls the size of change during updating the weights."))

(defgeneric transform (instance &key)
  (:documentation "@arg[instance]{an instance of @code{principal-component-analysis} or @code{imputer}}
@arg[components]{a number that states how many dimensions should be used for a transformation (default is @code{nil} which means that it should use all dimensions of @code{data-set}}
@arg[inverse]{do an inverse transformation (@code{t} or @code{nil}, default: @code{nil})}
@arg[new-data]{do the transformation on this data-set (default is @code{nil} which means, that it should use @code{data-set})}
@return{the transformed data-set}
Project some data on its principal components. / Fit missing values."))